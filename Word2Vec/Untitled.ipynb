{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Multithreaded word2vec'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# word2vec = tf.load_op_library(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'word2vec_ops.so'))\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string(\"save_path\", None, \"Directory to write the model and \"\n",
    "                    \"training summaries.\")\n",
    "flags.DEFINE_string(\"train_data\", None, \"Training text file. \"\n",
    "                    \"E.g., unzipped file http://mattmahoney.net/dc/text8.zip.\")\n",
    "flags.DEFINE_string(\n",
    "    \"eval_data\", None, \"File consisting of analogies of four tokens.\"\n",
    "    \"embedding 2 - embedding 1 + embedding 3 should be close \"\n",
    "    \"to embedding 4.\"\n",
    "    \"See README.md for how to get 'questions-words.txt'.\")\n",
    "flags.DEFINE_integer(\"embedding_size\", 200, \"The embedding dimension size.\")\n",
    "flags.DEFINE_integer(\n",
    "    \"epochs_to_train\", 15,\n",
    "    \"Number of epochs to train. Each epoch processes the training data once \"\n",
    "    \"completely.\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.2, \"Initial learning rate.\")\n",
    "flags.DEFINE_integer(\"num_neg_samples\", 100,\n",
    "                     \"Negative samples per training example.\")\n",
    "flags.DEFINE_integer(\"batch_size\", 16,\n",
    "                     \"Number of training examples processed per step \"\n",
    "                     \"(size of a minibatch).\")\n",
    "flags.DEFINE_integer(\"concurrent_steps\", 12,\n",
    "                     \"The number of concurrent training steps.\")\n",
    "flags.DEFINE_integer(\"window_size\", 5,\n",
    "                     \"The number of words to predict to the left and right \"\n",
    "                     \"of the target word.\")\n",
    "flags.DEFINE_integer(\"min_count\", 5,\n",
    "                     \"The minimum number of word occurrences for it to be \"\n",
    "                     \"included in the vocabulary.\")\n",
    "flags.DEFINE_float(\"subsample\", 1e-3,\n",
    "                   \"Subsample threshold for word occurrence. Words that appear \"\n",
    "                   \"with higher frequency will be randomly down-sampled. Set \"\n",
    "                   \"to 0 to disable.\")\n",
    "flags.DEFINE_boolean(\n",
    "    \"interactive\", False,\n",
    "    \"If true, enters an IPython interactive session to play with the trained \"\n",
    "    \"model. E.g., try model.analogy(b'france', b'paris', b'russia') and \"\n",
    "    \"model.nearby([b'proton', b'elephant', b'maxwell'])\")\n",
    "flags.DEFINE_integer(\"statistics_interval\", 5,\n",
    "                     \"Print statistics every n seconds.\")\n",
    "flags.DEFINE_integer(\"summary_interval\", 5,\n",
    "                     \"Save training summary to file every n seconds (rounded \"\n",
    "                     \"up to statistics interval).\")\n",
    "flags.DEFINE_integer(\"checkpoint_interval\", 600,\n",
    "                     \"Checkpoint the model (i.e. save the parameters) every n \"\n",
    "                     \"seconds (rounded up to statistics interval).\")\n",
    "\n",
    "FLAGS = tf.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Options(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.emb_dim = FLAGS.embedding_size\n",
    "        self.train_data = FLAGS.train_data\n",
    "        self.num_samples = FLAGS.num_neg_samples\n",
    "        self.learning_rate = FLAGS.learning_rate\n",
    "        self.epochs_to_train = FLAGS.epochs_to_train\n",
    "        self.concurrent_steps = FLAGS.concurrent_steps\n",
    "        self.batch_size = FLAGS.batch_size\n",
    "        self.window_size = FLAGS.window_size\n",
    "        self.min_count = FLAGS.min_count\n",
    "        self.subsample = FLAGS.subsample\n",
    "        self.statistics_interval = FLAGS.statistics_interval\n",
    "        self.summary_interval = FLAGS.summary_interval\n",
    "        self.checkpoint_interval = FLAGS.checkpoint_interval\n",
    "\n",
    "        self.save_path = FLAGS.save_path\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "        self.eval_data = FLAGS.eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "\n",
    "    def __init__(self, options, session):\n",
    "        self._options = options\n",
    "        self._session = session\n",
    "        self._word2id = {}\n",
    "        self._id2word = []\n",
    "        self.buildgraph()\n",
    "        self.build_eval_graph()\n",
    "        self.save_vocab()\n",
    "\n",
    "    def read_analogies(self):\n",
    "        questions = []\n",
    "        questions_skipped = 0\n",
    "        with open(self._options.eval_data, 'rb') as analogy_f:\n",
    "            for line in analogy_f:\n",
    "                if line.startswith(':'):\n",
    "                    continue\n",
    "                else:\n",
    "                    words = line.strip().lower.split(\" \")\n",
    "                    ids = [self._word2id.get(w.strip()) for w in words]\n",
    "                    if None in ids or len(ids) != 4:\n",
    "                        questions_skipped += 1\n",
    "                    else:\n",
    "                        questions.append(np.array(ids))\n",
    "        print(\"Eval analogy file: \", self._options.eval_data)\n",
    "        print(\"Questions: \", len(questions))\n",
    "        print(\"Skipped: \", questions_skipped)\n",
    "        self._analogy_questions = np.array(questions, dtype=np.int32)\n",
    "\n",
    "\n",
    "    def forward(self, examples, labels):\n",
    "        opts = self._options\n",
    "\n",
    "        # embedding: [vocab_size, emb_dim]\n",
    "        init_width = 0.5 / opts.emb_dim\n",
    "        emb = tf.Variable(\n",
    "            tf.random_uniform(\n",
    "                [opts.vocab_size, opts.emb_dim],\n",
    "                -init_width, init_width),\n",
    "            name='emb')\n",
    "        self._emb = emb\n",
    "\n",
    "        # softmax weight: [vocab_size, emb_dim].T\n",
    "        sm_w_t = tf.Variable(\n",
    "            tf.zeros([opts.vocab_size, opts.embedding_size]), \n",
    "            name='sw_m_t')\n",
    "\n",
    "        # softmax bias: [vocab_size]\n",
    "        sm_b = tf.Variable(tf.zeros(opts.vocab_size), name='sm_b')\n",
    "\n",
    "        # global step\n",
    "        self.global_step = tf.Variable(0, name='global_step')\n",
    "\n",
    "        # nodes to compute the NCE loss w/ candidate sampling\n",
    "        labels_matrix = tf.reshape(\n",
    "            tf.cast(labels, dtype=tf.int64),\n",
    "        [opts.batch_size, 1])\n",
    "        \n",
    "        # Negative Sampling\n",
    "        sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler(\n",
    "            true_classes=labels_matrix, \n",
    "            num_true=1,\n",
    "            num_sampled=opts.vocab_size,\n",
    "            distortion=0.75,\n",
    "            unigram=opts.vocab_size.tolist()))\n",
    "        \n",
    "        # Embeedings for examples: [batch_size, emb_dim]\n",
    "        example_emb = tf.nn.embedding_lookup(emb, examples)\n",
    "        \n",
    "        # Biases for labels: [batch_size, 1]\n",
    "        true_w = tf.nn.embedding_lookup(sm_w_t, examples)\n",
    "        # Biases for labels: []\n",
    "        true_b = tf.nn.embedding_lookup(sm_b, labels)\n",
    "        \n",
    "        # weight for sampled ids: [num_samples, emb_dim]\n",
    "        sampled_w = tf.nn.embedding_lookup(sm_w_t, sampled_ids)\n",
    "        #biases for sampled id: [num_sampled, 1]\n",
    "        sampled_b = tf.nn.embedding_lookup(sm_b, sampled_ids)\n",
    "        \n",
    "        # True logits: [batch_size, num_sampled]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Neural Language Probabilistic Language Model specifies rge distribution for the target words given the a sequence of words $h$ from content  \n",
    "  \n",
    "$$ P_{\\theta}^{h}(w) = \\frac{exp(s_\\theta (w,h)} {\\sum exp(s_\\theta (w, h))}$$\n",
    "\n",
    "This equation is intractable, and three solutions are presented:\n",
    "1. tree structured vocabulary --> nontrival\n",
    "2. importance sampling --> instable\n",
    "3. NCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable Log-Bilinear Models\n",
    "let $q_w$ and $\\gamma_w$ be the target and content representations for word $w$, given a sequence of context words $h = w_1, ..., w_n$, the model computes the predicted representation for the target word by $\\widehat{q} = \\sum c_i \\odot \\gamma_w$ where $c_i$ is the weight, and $s_\\theta(w,h) = \\widehat{q}_h ^T q_w + b_w$  \n",
    "  \n",
    "As our main concern is learning word representations, we are free to move away from the paradigm of predicting the target from context and do the reverse. This is motivated by the **distributed hypothesis**, words with similar meaning often occur in similar contexts. Thus we'll be looking for word representations that capture the context distributions -- to predict context from words\n",
    "\n",
    "Unfortunately, predicting n-word context requires modeling the joing distribution of n-words. This is considerably harder than modeling one of the words. We can make this trackable by assuming words in different context positions are conditionally independent: given current word $w$\n",
    "\n",
    "$$P_\\theta^w(h) = \\prod p_{i,\\theta}^w(w_i) $$\n",
    "$$s_{i,\\theta}(w_i,w) = (c_i \\odot \\gamma_w)^T q_{w_i} + b_{w_i}$$\n",
    "\n",
    "where the $c_i$, position-specific weight is optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
